# Local-LLM

# ğŸ¤– RAG Comparison System

A modern Retrieval-Augmented Generation (RAG) system that compares **LangChain** and **LlamaIndex** implementations side-by-side. Perfect for understanding different RAG approaches and choosing the right framework for your project.

## âœ¨ Features

- **Three RAG Implementations:**
  - LangChain Manual Pipeline (full control)
  - LangChain RetrievalQA Chain (high-level abstraction)
  - LlamaIndex (optimized RAG framework)

- **CSV Document Processing:** Load and query structured data from CSV files
- **Vector Storage:** FAISS-based vector database with persistence
- **Powered by Groq:** Fast LLM inference with Llama 3.3 70B
- **Side-by-Side Comparison:** Run all three methods with one query

## ğŸš€ Quick Start

### Prerequisites

```bash
python >= 3.8
```

### Installation

1. Clone the repository:
```bash
git clone https://github.com/MohamedIbrahim-20/rag-comparison.git
cd rag-comparison
```

2. Set up your environment variables:
```bash
cp .env.example .env
# Edit .env and add your GROQ_API_KEY
```

3. Prepare your data:
```bash
# Place your CSV file in the project root
# Default: tech_100_long_real.csv
```

### Usage

Run the comparison system:

```bash
python rag.py
```

Enter your question when prompted, and the system will:
1. Load your documents
2. Create/load vector embeddings
3. Run all three RAG methods
4. Display answers from each approach

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ rag.py                          # Main script
â”œâ”€â”€ tech_100_long_real.csv         # Sample data (your CSV here)
â”œâ”€â”€ .env                           # API keys
â”œâ”€â”€ requirements.txt               # Dependencies
â””â”€â”€ storage/
    â”œâ”€â”€ langchain_faiss/           # LangChain vector store
    â””â”€â”€ llamaindex_storage/        # LlamaIndex vector store
```

## ğŸ”§ Configuration

Edit the configuration section in `rag.py`:

```python
# Model Configuration
LLM_MODEL = "llama-3.3-70b-versatile"
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# RAG Parameters
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 100
TOP_K_RESULTS = 5
TEMPERATURE = 0.1

# CSV Configuration
CSV_FILE = "./tech_100_long_real.csv"
```

## ğŸ“Š CSV Format

Your CSV should contain at least:
- `title`: Document title
- `content`: Main content
- Additional metadata columns (optional)

Example:
```csv
title,content,category,date
"Python Best Practices","Write clean, maintainable code...",programming,2024-01-01
```

## ğŸ› ï¸ Dependencies

```
langchain
langchain-community
langchain-groq
langchain-huggingface
llama-index
llama-index-llms-groq
llama-index-embeddings-huggingface
faiss-cpu
pandas
python-dotenv
```

## ğŸ”‘ API Keys

Get a free Groq API key:
1. Visit [console.groq.com](https://console.groq.com)
2. Sign up for an account
3. Generate an API key
4. Add to `.env`: `GROQ_API_KEY=your_key_here`

## ğŸ“ Example Output

```
ğŸ¤– RAG COMPARISON SYSTEM
============================================================
Model: llama-3.3-70b-versatile
Embeddings: sentence-transformers/all-MiniLM-L6-v2
Chunk Size: 1000 | Overlap: 100
Top-K Results: 5
============================================================

â“ Enter your question: What are the latest developments in AI?

============================================================
ğŸ”§ LangChain Manual RAG Pipeline
============================================================
ğŸ“„ Split into 150 chunks
ğŸ“‚ Loading existing vector store
ğŸ” Retrieved 5 relevant chunks

ğŸ’¡ Answer:
[Answer from LangChain Manual method...]

============================================================
âš¡ LangChain RetrievalQA Chain
============================================================
ğŸ“„ Split into 150 chunks

ğŸ’¡ Answer:
[Answer from LangChain QA Chain...]

============================================================
ğŸš€ LlamaIndex RAG
============================================================
ğŸ“‚ Loading existing index

ğŸ’¡ Answer:
[Answer from LlamaIndex...]

âœ… All RAG methods completed successfully!
```


## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.
